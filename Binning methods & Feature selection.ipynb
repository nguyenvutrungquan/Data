{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8fbcf1e",
   "metadata": {},
   "source": [
    "# How to use this file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a770f1",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "All functions have the form\n",
    "\n",
    "function(df, column_name, bins = None, method = None, target = None)\n",
    "\n",
    "There are FIVE groups of functions:\n",
    "\n",
    "    - Group 1: For each feature, extract the list of bins.\n",
    "    - Group 2: For each feature, extract the distribution corresponding to the list of bins.\n",
    "    - Group 3: For each feature, extract the bad rate corresponding to each bin in the list of bins. The 'target' argument must be assigned if using the functions in this group.\n",
    "    - Group 4: For each feature, extract the result whether it is needed to re-bin.\n",
    "    - Group 5: For each feature, perform re-binning by grouping categorical bins together and extract the new distribution table and the new bad rate table.\n",
    "\n",
    "There are THREE choices to use these functions:\n",
    "\n",
    "    - method == None: You choose bins on your own.\n",
    "    - method == 'optimal_binning': You use the Optimal Binning method.\n",
    "    - method == 'statistics': You use the Statistics method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b3488",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Group 1: Extract List of bins:\n",
    "\n",
    "binning_list(df, 'max_dpdall_ref', bins = [0,1])\n",
    "\n",
    "binning_list_new(df, 'max_dpdall_ref', method = 'statistics')\n",
    "\n",
    "binning_list_plus(df, 'max_dpdall_ref', method = 'optimal_binning', target = 'del90_mob12_app')\n",
    "\n",
    "ranking_list(df, 'max_dpdall_ref', method = 'optimal_binning', target = 'del90_mob12_app')\n",
    "\n",
    "### Group 2: Extract Distribution array, Distribution table, and plot Distribution bar chart\n",
    "\n",
    "count_binning_final(df, 'max_dpdall_ref', bins = [0,1])\n",
    "\n",
    "distribution_table_final(df, 'max_dpdall_ref', method = 'statistics')\n",
    "\n",
    "distribution_plotting(df, 'max_dpdall_ref', method = 'optimal_binning', target = 'del90_mob12_app')\n",
    "\n",
    "### Group 3: Extract Bad rate array, Bad rate table, and plot Bad rate line chart\n",
    "\n",
    "del_mob_percent_final(df, 'max_dpdall_ref', bins = [0,1])\n",
    "\n",
    "del_mob_table_final(df, 'max_dpdall_ref', method = 'statistics')\n",
    "    \n",
    "del_mob_plotting(df, 'max_dpdall_ref', method = 'optimal_binning', target = 'del90_mob12_app')\n",
    "\n",
    "### Group 4: Decide whether it is needed to conduct re-binning for a feature\n",
    "\n",
    "Note that the 'target' argument MUST NOT be None.\n",
    "\n",
    "is_feature_need_to_be_rebinned(df, 'max_dpdall_ref', bins = [0,1], target = 'del90_mob12_app')\n",
    "\n",
    "is_feature_need_to_be_rebinned(df, 'max_dpdall_ref', method = 'statistics', target = 'del90_mob12_app')\n",
    "\n",
    "is_feature_need_to_be_rebinned(df, 'max_dpdall_ref', method = 'optimal_binning', target = 'del90_mob12_app')\n",
    "\n",
    "### Group 5: Group categorical values and extract the updated tables of Distribution and Bad rate\n",
    "\n",
    "The 'bins' argument MUST be a list of list. And the 'target' argument MUST NOT be None.\n",
    "\n",
    "bin1 = [['X-sell 2ND', 'X-sell CDL'], ['X-sell PL'], ['X-sell TW'], ['unknown']]\n",
    "group_categorical_values(df, 'product_group_dt2', bins = bin1, method = 'statistics', target = 'del90_mob12_app')\n",
    "\n",
    "bin2 = [['<= 0'], ['<= 1', '> 1'], ['unknown']]\n",
    "group_categorical_values(df, list_column[4], bins = bin2, method = 'optimal_binning', target = 'del90_mob12_app')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3bcbc",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532291f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import cx_Oracle\n",
    "import pandas as pd\n",
    "import math as m\n",
    "import xlsxwriter\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "#import polars as pl\n",
    "import plotly.graph_objects as go\n",
    "#import risk_mdl\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "from optbinning import OptimalBinning, BinningProcess\n",
    "pd.set_option('mode.chained_assignment', None) # Hide warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b5a748e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "connection = cx_Oracle.connect(user = **********, password = *********,  dsn = **********)\n",
    "cursor = connection.cursor()\n",
    "sql_command = '''\n",
    "select A.*,\n",
    "       ------------------------------------------------------------\n",
    "       (case when\n",
    "            (to_date(concat((substr(to_char(applied_date, 'YYYY/'), 1, 5)),'01/01'), 'yyyy/mm/dd') between (applied_date -5)  and (applied_date + 5))\n",
    "       or   (to_date(concat((substr(to_char(applied_date, 'YYYY/'), 1, 5)),'02/14'), 'yyyy/mm/dd') between (applied_date -5)  and (applied_date + 5))\n",
    "       or   (to_date(concat((substr(to_char(applied_date, 'YYYY/'), 1, 5)),'03/08'), 'yyyy/mm/dd') between (applied_date -5)  and (applied_date + 5))\n",
    "       or   (to_date(concat((substr(to_char(applied_date, 'YYYY/'), 1, 5)),'04/30'), 'yyyy/mm/dd') between (applied_date -5)  and (applied_date + 5))\n",
    "       or   (to_date(concat((substr(to_char(applied_date, 'YYYY/'), 1, 5)),'05/01'), 'yyyy/mm/dd') between (applied_date -5)  and (applied_date + 5))\n",
    "       or   (to_date(concat((substr(to_char(applied_date, 'YYYY/'), 1, 5)),'06/01'), 'yyyy/mm/dd') between (applied_date -5)  and (applied_date + 5))\n",
    "       or   (to_date(concat((substr(to_char(applied_date, 'YYYY/'), 1, 5)),'09/02'), 'yyyy/mm/dd') between (applied_date -5)  and (applied_date + 5))\n",
    "       or   (to_date(concat((substr(to_char(applied_date, 'YYYY/'), 1, 5)),'09/05'), 'yyyy/mm/dd') between (applied_date -5)  and (applied_date + 5))\n",
    "       or   (to_date(concat((substr(to_char(applied_date, 'YYYY/'), 1, 5)),'10/20'), 'yyyy/mm/dd') between (applied_date -5)  and (applied_date + 5))\n",
    "       or   (to_date(concat((substr(to_char(applied_date, 'YYYY/'), 1, 5)),'11/20'), 'yyyy/mm/dd') between (applied_date -5)  and (applied_date + 5))\n",
    "       or   (to_date(concat((substr(to_char(applied_date, 'YYYY/'), 1, 5)),'12/25'), 'yyyy/mm/dd') between (applied_date -5)  and (applied_date + 5))\n",
    "       then 1\n",
    "       else 0\n",
    "       end\n",
    "       ) as holiday_flag,\n",
    "       --------------------------\n",
    "       (case when\n",
    "            to_date(concat\n",
    "                     (\n",
    "                     (substr(to_char(applied_date, 'YYYY/MM/'), 1, 7)),\n",
    "                     (substr(to_char(applied_date, 'YYYY/MM'), 6, 7))              \n",
    "                     ), 'yyyy/mm/dd') between (applied_date -3)  and (applied_date + 3)\n",
    "       then 1\n",
    "       else 0\n",
    "       end\n",
    "       ) as double_day_flag,\n",
    "       --------------------------\n",
    "       to_char(applied_date, 'day') as day_of_week,\n",
    "       case when to_char(applied_date, 'd') in (1,7) then 1 else 0 end as weekend_flag\n",
    "from quan_tbl_final_all A\n",
    "'''\n",
    "df = pd.read_sql_query(sql_command, con= connection)\n",
    "\n",
    "df.columns = df.columns.str.lower()\n",
    "list_column = df.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122b32f2",
   "metadata": {},
   "source": [
    "# Fix some negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a89d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['max_dpdall_ref'] = df['max_dpdall_ref'].mask(df['max_dpdall_ref'] < 0).fillna(np.nan)\n",
    "df['max_dpd_ref'] = df['max_dpd_ref'].mask(df['max_dpd_ref'] < 0).fillna(np.nan)\n",
    "df['months_dpd_0_cus_journey'] = df['months_dpd_0_cus_journey'].mask(df['months_dpd_0_cus_journey'] < 0).fillna(np.nan)\n",
    "df['months_dpd_10_cus_journey'] = df['months_dpd_10_cus_journey'].mask(df['months_dpd_10_cus_journey'] < 0).fillna(np.nan)\n",
    "df['months_dpd_30_cus_journey'] = df['months_dpd_30_cus_journey'].mask(df['months_dpd_30_cus_journey'] < 0).fillna(np.nan)\n",
    "df['avg_enr_ratio_ever'] = df['avg_enr_ratio_ever'].mask(df['avg_enr_ratio_ever'] < 0).fillna(np.nan)\n",
    "df['avg_enr_ratio_last_3m'] = df['avg_enr_ratio_last_3m'].mask(df['avg_enr_ratio_last_3m'] < 0).fillna(np.nan)\n",
    "df['avg_enr_ratio_last_3_6m'] = df['avg_enr_ratio_last_3_6m'].mask(df['avg_enr_ratio_last_3_6m'] < 0).fillna(np.nan)\n",
    "df['avg_enr_ratio_last_6_12m'] = df['avg_enr_ratio_last_6_12m'].mask(df['avg_enr_ratio_last_6_12m'] < 0).fillna(np.nan)\n",
    "df['avg_enr_ratio_last_12_24m'] = df['avg_enr_ratio_last_12_24m'].mask(df['avg_enr_ratio_last_12_24m'] < 0).fillna(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dbe669",
   "metadata": {},
   "source": [
    "# Binning choices and Classifications (Old)\n",
    "################################################# Statistics method\n",
    "binning_choice_S1 = ['cif_nb', 'app_id', 'applied_date', 'agreement_no', \n",
    "                     'province', 'rownum_', 'applied_month', 'applied_month_xsell'\n",
    "                    'deal_no', 'disbursal_dt'] #Do nothing\n",
    "binning_choice_S2 = ['max_appregregion', 'min_appregregion', 'mode_appregregion',\n",
    "                     'max_appresregion', 'min_appresregion', 'mode_appresregion'] #Special binning methods\n",
    "binning_choice_Y = [] #Yes - Need binning methods\n",
    "binning_choice_YS = [] #Yes with special treament\n",
    "binning_choice_N = ['day_of_week'] #No - No need binning methods\n",
    "\n",
    "for i in range(1, len(df_group_column)):\n",
    "    if df_group_column['binning_choice_statistics'][i] == \"Y\":\n",
    "        binning_choice_Y.append(df_group_column['column_name'][i])\n",
    "    elif df_group_column['binning_choice_statistics'][i] == \"YS\":\n",
    "        binning_choice_YS.append(df_group_column['column_name'][i])\n",
    "    elif df_group_column['binning_choice_statistics'][i] == \"N\":\n",
    "        binning_choice_N.append(df_group_column['column_name'][i])\n",
    "        \n",
    "################################################# Optimal Binning method          \n",
    "optimal_binning_S1 = ['cif_nb', 'app_id', 'applied_date', 'agreement_no', \n",
    "                     'province', 'rownum_', 'applied_month',\n",
    "                     'deal_no', 'disbursal_dt',\n",
    "                     'app_id_xsell', 'app_status_xsell', \n",
    "                     'product_group'] #Do nothing\n",
    "optimal_binning_S2 = ['max_appregregion', 'min_appregregion', 'mode_appregregion',\n",
    "                      'max_appresregion', 'min_appresregion', 'mode_appresregion'] #Special binning methods\n",
    "optimal_binning_N = ['day_of_week'] #Added 'day_of_week' feature\n",
    "optimal_binning_Y_int = []\n",
    "optimal_binning_Y_float = []\n",
    "\n",
    "for column in list_column:\n",
    "    if column in optimal_binning_S1:\n",
    "        df[column] = df[column].fillna(value = 'unknown')\n",
    "    elif column in optimal_binning_S2:\n",
    "        df[column] = df[column].fillna(value = 'unknown')\n",
    "    else:\n",
    "        if df[column].dtype == 'object':\n",
    "            optimal_binning_N.append(column)\n",
    "            df[column] = df[column].fillna(value = 'unknown')\n",
    "        elif df[column].dtype == 'int64':\n",
    "            optimal_binning_Y_int.append(column)\n",
    "            df[column] = df[column].fillna(value = np.nan)\n",
    "        elif df[column].dtype == 'float64':\n",
    "            optimal_binning_Y_float.append(column)\n",
    "            df[column] = df[column].fillna(value = np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808aba31",
   "metadata": {},
   "source": [
    "# Binning choices and Classification (new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2df3597",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_month = df['applied_month_xsell'].unique().tolist()\n",
    "list_of_month = sorted(list_of_month, reverse = False)\n",
    "\n",
    "# Group column and binning choice\n",
    "df_group_column = pd.read_excel('distinct_value2.xlsx')\n",
    "\n",
    "#############################################\n",
    "groupA = [] #application \n",
    "groupB = [] #cic\n",
    "groupC = [] #behavior\n",
    "groupD = [] #collection feedback\n",
    "groupE = [] #linkage\n",
    "\n",
    "for i in range(1, len(list_column)):\n",
    "    if (df_group_column['group'][i] == \"A\") or (df_group_column['group'][i] == \"Another\"):\n",
    "        groupA.append(list_column[i])\n",
    "    elif df_group_column['group'][i] == \"B\":\n",
    "        groupB.append(list_column[i])\n",
    "    elif df_group_column['group'][i] == \"C\":\n",
    "        groupC.append(list_column[i])\n",
    "    elif df_group_column['group'][i] == \"D\":\n",
    "        groupD.append(list_column[i])\n",
    "    elif df_group_column['group'][i] == \"E\":\n",
    "        groupE.append(list_column[i])\n",
    "\n",
    "#################################################\n",
    "binning_choice_S1 = [] #Do nothing\n",
    "binning_choice_S2 = [] #Special binning methods\n",
    "binning_choice_Y = [] #Yes - Need binning methods\n",
    "binning_choice_YS = [] #Yes with special treament\n",
    "binning_choice_N =[]\n",
    "\n",
    "optimal_binning_S1 = [] #Do nothing\n",
    "optimal_binning_S2 = [] #Special binning methods\n",
    "optimal_binning_N = [] #Added 'day_of_week' feature\n",
    "optimal_binning_Y_int = []\n",
    "optimal_binning_Y_float = []\n",
    "for column in list_column:\n",
    "    idx = df_group_column[df_group_column[\"column_name\"] == column].index.tolist()[0]\n",
    "    if df_group_column['binning_choice_statistics'][idx] == \"Y\":\n",
    "        binning_choice_Y.append(column)\n",
    "    elif df_group_column['binning_choice_statistics'][idx] == \"YS\":\n",
    "        binning_choice_YS.append(column)\n",
    "    elif df_group_column['binning_choice_statistics'][idx] == \"N\":\n",
    "        binning_choice_N.append(column)\n",
    "    elif df_group_column['binning_choice_statistics'][idx] == \"S1\":\n",
    "        binning_choice_S1.append(column)\n",
    "    elif df_group_column['binning_choice_statistics'][idx] == \"S2\":\n",
    "        binning_choice_S2.append(column)\n",
    "    \n",
    "    if df_group_column['binning_choice_optimal'][idx] == \"Y\":\n",
    "        if df[column].dtype == 'int64':\n",
    "            optimal_binning_Y_int.append(column)\n",
    "            df[column] = df[column].fillna(value = np.nan)\n",
    "        elif df[column].dtype == 'float64':\n",
    "            optimal_binning_Y_float.append(column)\n",
    "            df[column] = df[column].fillna(value = np.nan)\n",
    "    elif df_group_column['binning_choice_optimal'][idx] == \"N\":\n",
    "        optimal_binning_N.append(df_group_column['column_name'][idx])\n",
    "        df[column] = df[column].fillna(value = 'unknown')\n",
    "    elif df_group_column['binning_choice_optimal'][idx] == \"S1\":\n",
    "        optimal_binning_S1.append(df_group_column['column_name'][idx])\n",
    "        df[column] = df[column].fillna(value = 'unknown')\n",
    "    elif df_group_column['binning_choice_optimal'][idx] == \"S2\":\n",
    "        optimal_binning_S2.append(df_group_column['column_name'][idx])\n",
    "        df[column] = df[column].fillna(value = 'unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc340e",
   "metadata": {},
   "source": [
    "# Binning List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "392cc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning_list(df, column_name, bins = None, method = None, target = None):\n",
    "    list_of_bin = []\n",
    "    #\n",
    "    if method == None:\n",
    "        list_of_bin = [-10**9] + bins + [10**9]\n",
    "        return list_of_bin\n",
    "    #\n",
    "    elif method == 'statistics':\n",
    "        #\n",
    "        if column_name in binning_choice_N:\n",
    "            list_of_bin = df[column_name].unique().tolist()\n",
    "            if 'blank' in list_of_bin:\n",
    "                list_of_bin.remove('blank')\n",
    "            if 'null' in list_of_bin:\n",
    "                list_of_bin.remove('null')\n",
    "            if 'unknown' in list_of_bin:\n",
    "                list_of_bin.remove('unknown')\n",
    "            if np.nan in list_of_bin:\n",
    "                list_of_bin.remove(np.nan)\n",
    "            if None in list_of_bin:\n",
    "                list_of_bin.remove(None)\n",
    "            list_of_bin = sorted(list_of_bin, reverse = False)\n",
    "        #\n",
    "        elif column_name in binning_choice_S1:\n",
    "            return 'Not suitable for binning'\n",
    "        #\n",
    "        elif column_name in binning_choice_S2:\n",
    "            list_of_bin = ['big_city', 'another_city']\n",
    "        #\n",
    "        elif column_name in binning_choice_Y:\n",
    "            standard_deviation = np.std(df[df[column_name] != np.nan][column_name])\n",
    "            average = np.mean(df[df[column_name] != np.nan][column_name])\n",
    "            a = average - 1.282*standard_deviation\n",
    "            b = average - 0.674*standard_deviation\n",
    "            c = average - 0.385*standard_deviation\n",
    "            d = average + 0.385*standard_deviation\n",
    "            e = average + 0.842*standard_deviation\n",
    "            f = average + 1.282*standard_deviation\n",
    "            list_of_bin = [-10**9]\n",
    "            for i in np.unique([a,b,c,d,e,f]).tolist():\n",
    "                list_of_bin.append(round(i,5))\n",
    "            list_of_bin.append(10**9)\n",
    "        #\n",
    "        elif column_name in binning_choice_YS:\n",
    "            standard_deviation = np.std(df[df[column_name] != np.nan][column_name])\n",
    "            average = np.mean(df[df[column_name] != np.nan][column_name])\n",
    "            a = average - 1.282*standard_deviation\n",
    "            b = average - 0.674*standard_deviation\n",
    "            c = average + 0.674*standard_deviation\n",
    "            d = average + 1.282*standard_deviation\n",
    "            list_of_bin = [-10**9]\n",
    "            for i in np.unique([a,b,c,d]).tolist():\n",
    "                list_of_bin.append(round(i,5))\n",
    "            list_of_bin.append(10**9)\n",
    "        \n",
    "        return list_of_bin\n",
    "    #\n",
    "    elif method == 'optimal_binning':\n",
    "        #\n",
    "        variable = column_name\n",
    "        y = df[target]\n",
    "        list_of_bin = []\n",
    "        if column_name in optimal_binning_S1:\n",
    "            return list_of_bin\n",
    "        #\n",
    "        elif column_name in optimal_binning_S2:\n",
    "            list_of_bin = ['big_city', 'another_city']\n",
    "            return list_of_bin\n",
    "        #\n",
    "        elif column_name in optimal_binning_N:\n",
    "            list_of_bin = df[column_name].unique().tolist()\n",
    "            if 'blank' in list_of_bin:\n",
    "                list_of_bin.remove('blank')\n",
    "            if 'null' in list_of_bin:\n",
    "                list_of_bin.remove('null')\n",
    "            if 'unknown' in list_of_bin:\n",
    "                list_of_bin.remove('unknown')\n",
    "            if np.nan in list_of_bin:\n",
    "                list_of_bin.remove(np.nan)\n",
    "            if None in list_of_bin:\n",
    "                list_of_bin.remove(None)\n",
    "            list_of_bin = sorted(list_of_bin, reverse = False)\n",
    "            return list_of_bin\n",
    "        #\n",
    "        elif column_name in optimal_binning_Y_int:\n",
    "            list_of_bin = [-10**9]\n",
    "            var = [variable]\n",
    "            binning_process = BinningProcess(variable_names=var, \n",
    "                                         #max_n_bins=5,\n",
    "                                         #min_prebin_size = 0.2,\n",
    "                                         split_digits=4)\n",
    "            binning_process.fit(df[df[variable] != np.nan][var], y)\n",
    "            a = sorted(np.unique(binning_process.transform(df[df[variable] != np.nan][var]).values).tolist(),\n",
    "                       reverse = False)\n",
    "            for i in a:\n",
    "                list_of_bin.append(m.ceil(i))\n",
    "            list_of_bin.append(10**9)\n",
    "            return np.unique(list_of_bin).tolist()\n",
    "        #\n",
    "        elif column_name in optimal_binning_Y_float:\n",
    "            list_of_bin = [-10**9]\n",
    "            var = [variable]\n",
    "            binning_process = BinningProcess(variable_names=var, \n",
    "                                         #max_n_bins=5,\n",
    "                                         #min_prebin_size = 0.2,\n",
    "                                         split_digits=4)\n",
    "            binning_process.fit(df[df[variable] != np.nan][var], y)\n",
    "            a = sorted(np.unique(binning_process.transform(df[df[variable] != np.nan][var]).values).tolist(),\n",
    "                       reverse = False)\n",
    "            for i in a:\n",
    "                list_of_bin.append(round(i,5))\n",
    "            list_of_bin.append(10**9)\n",
    "            return np.unique(list_of_bin).tolist()\n",
    "\n",
    "def binning_list_new(df, column_name, bins = None, method = None, target = None):\n",
    "    list_of_bin = binning_list(df, column_name, bins, method, target)\n",
    "    #\n",
    "    if method == 'statistics':\n",
    "        #\n",
    "        if (column_name in binning_choice_S1) or (column_name in binning_choice_S2) or (column_name in binning_choice_N):\n",
    "            return list_of_bin\n",
    "        #\n",
    "        elif column_name in binning_choice_YS:\n",
    "            label = []\n",
    "            category = ['very_low','low', 'medium', 'high', 'very_high']\n",
    "            for k in range(len(list_of_bin)-2):\n",
    "                label.append('<= ' + str(list_of_bin[k+1]) + ' - ' + category[k])\n",
    "            label.append('> ' + str(list_of_bin[len(list_of_bin)-2]) + ' - ' + category[len(list_of_bin)-2])\n",
    "            return label\n",
    "        #\n",
    "        elif column_name in binning_choice_Y:\n",
    "            label = []\n",
    "            category = ['extremely_low','very_low', 'low', 'medium', 'high', 'very_high', 'extremely_high']\n",
    "            for k in range(len(list_of_bin)-2):\n",
    "                label.append('<= ' + str(list_of_bin[k+1]) + ' - ' + category[k])\n",
    "            label.append('> ' + str(list_of_bin[len(list_of_bin)-2]) + ' - ' + category[len(list_of_bin)-2])\n",
    "            return label\n",
    "    #\n",
    "    elif (method == 'optimal_binning') or (method == None):\n",
    "        if (column_name in optimal_binning_S1) or (column_name in optimal_binning_S2) or (column_name in optimal_binning_N):\n",
    "            return list_of_bin\n",
    "        else:\n",
    "            label = []\n",
    "            for k in range(len(list_of_bin)-2):\n",
    "                label.append('<= ' + str(list_of_bin[k+1]))\n",
    "            label.append('> ' + str(list_of_bin[len(list_of_bin)-2]))\n",
    "            return label\n",
    "        \n",
    "def ranking_list(df, column_name, bins = None, method = None, target = None): #Only for S2, Y_int, Y_float, Y, YS types\n",
    "    ranking_list = []\n",
    "    if column_name in optimal_binning_S1:\n",
    "        return 'Not suitable for ranking'\n",
    "    elif column_name in optimal_binning_S2:\n",
    "        for i in range(len(df[column_name])):\n",
    "            if df[column_name][i] == 'unknown':\n",
    "                ranking_list.append('unknown')\n",
    "            elif df[column_name][i] < 60:\n",
    "                ranking_list.append('another_city')\n",
    "            elif df[column_name][i] >=60:\n",
    "                ranking_list.append('big_city')\n",
    "        return ranking_list\n",
    "    elif (column_name in optimal_binning_N) or (column_name in binning_choice_N):\n",
    "        return 'Not suitable for ranking'\n",
    "    else:\n",
    "        list_of_bin = binning_list(df, column_name, bins, method, target)\n",
    "        label = binning_list_new(df, column_name, bins, method, target)\n",
    "        ranking_list = pd.cut(np.array(df[column_name]), list_of_bin, labels= label)\n",
    "        return ranking_list\n",
    "\n",
    "def binning_list_plus(df, column_name, bins = None, method = None, target = None): #Only for Y and YS types\n",
    "    if (column_name in binning_choice_S1) or (column_name in binning_choice_S2) or (column_name in binning_choice_N) or (column_name in optimal_binning_N):\n",
    "        return binning_list(df, column_name, bins, method, target)\n",
    "    else:\n",
    "        a = ranking_list(df, column_name, bins, method, target).unique().tolist()\n",
    "        b = binning_list_new(df, column_name, bins, method, target)\n",
    "        c = []\n",
    "        for i in b:\n",
    "            if i in a:\n",
    "                c.append(i)\n",
    "        return c\n",
    "\n",
    "def binning_list_final(df, column_name, bins = None, method = None, target = None):\n",
    "    list_of_bin = binning_list_plus(df, column_name, bins, method, target)\n",
    "    list_of_bin.append('unknown')\n",
    "    return list_of_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a77de",
   "metadata": {},
   "source": [
    "# Distribution Table and Distribution Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98b7cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_binning_N(df, column_name, bins = None, method = None, target = None): #Only for type N\n",
    "    ## Get table\n",
    "    list_of_bin = binning_list_plus(df, column_name, bins, method, target)\n",
    "    list_of_total_by_month = df.groupby('applied_month_xsell', dropna = False)\\\n",
    "                               .aggregate({'app_id': 'count'})\\\n",
    "                               .reset_index()\\\n",
    "                               ['app_id']\\\n",
    "                               .cumsum().tolist()\n",
    "    df2 = df[['app_id', 'applied_month_xsell', column_name]]\\\n",
    "                            .groupby(['applied_month_xsell', column_name], dropna = False)\\\n",
    "                            .aggregate({'app_id': 'count'})\\\n",
    "                            .reset_index()\n",
    "    df3 = df2.pivot(index = 'applied_month_xsell', columns = column_name, values = 'app_id')\\\n",
    "                    .fillna(value = 0)\n",
    "            \n",
    "    ## Get distribution\n",
    "    distribution = []\n",
    "    for binning in list_of_bin:\n",
    "        a = (np.array(df3[binning].cumsum().tolist())/(np.array(list_of_total_by_month)))\n",
    "        df3[binning] = a\n",
    "        distribution_bin = df3[binning].tolist()\n",
    "        distribution.append(distribution_bin)\n",
    "        \n",
    "    x = distribution\n",
    "    y = np.array([1]*len(x[0])) - sum(np.array(x[0:(len(x))])) #Distribution for 'unknown' class\n",
    "    y = y.tolist()\n",
    "    x.append(y)\n",
    "    \n",
    "    df3.loc[:,'unknown'] = y\n",
    "    \n",
    "    return [df3, x]\n",
    "\n",
    "def count_binning_other(df, column_name, bins = None, method = None, target = None): #For Special type 2, Y, YS\n",
    "    ## Get table\n",
    "    #list_of_bin = optimal_binning(df,column_name, target)\n",
    "    df2 = df[['app_id', 'applied_month_xsell', column_name]]\n",
    "    df2.loc[:,'ranking'] = ranking_list(df, column_name, bins, method, target)#.fillna(value = 'unknown')\n",
    "    list_of_bin = binning_list_plus(df, column_name, bins, method, target)\n",
    "    \n",
    "    ####################\n",
    "    list_of_total_by_month = df.groupby('applied_month_xsell', dropna = True)\\\n",
    "                               .aggregate({'app_id': 'count'})\\\n",
    "                               .reset_index()\\\n",
    "                               ['app_id']\\\n",
    "                               .cumsum().tolist()\n",
    "    df3 = df2[['app_id', 'applied_month_xsell', 'ranking']]\\\n",
    "                            .groupby(['applied_month_xsell', 'ranking'], dropna = False)\\\n",
    "                            .aggregate({'app_id': 'count'})\\\n",
    "                            .reset_index()\n",
    "    df4 = df3.pivot(index = 'applied_month_xsell', columns = 'ranking', values = 'app_id')\\\n",
    "                    .fillna(value = 0)\n",
    "            \n",
    "    ## Get distribution\n",
    "    distribution = []\n",
    "    for binning in list_of_bin:\n",
    "        a = (np.array(df4[binning].cumsum().tolist())/(np.array(list_of_total_by_month)))\n",
    "        df4.loc[:, binning] = a\n",
    "        distribution_bin = df4[binning].tolist()\n",
    "        distribution.append(distribution_bin)\n",
    "    \n",
    "    x = distribution\n",
    "    y = np.array([1]*len(x[0])) - sum(np.array(x[0:(len(x))]))\n",
    "    y = y.tolist()\n",
    "    x.append(y)\n",
    "    \n",
    "    df4.loc[:,'unknown'] = y\n",
    "    \n",
    "    return [df4,x]\n",
    "\n",
    "def count_binning_final(df, column_name, bins = None, method = None, target = None):\n",
    "    if (column_name in binning_choice_N) or (column_name in optimal_binning_N):\n",
    "        return count_binning_N(df, column_name, bins, method, target)[1]\n",
    "    elif column_name in binning_choice_S1:\n",
    "        return 'Do nothing'\n",
    "    else:\n",
    "        return count_binning_other(df, column_name, bins, method, target)[1]\n",
    "\n",
    "def distribution_table_final(df, column_name, bins = None, method = None, target = None):\n",
    "    if (column_name in binning_choice_N) or (column_name in optimal_binning_N):\n",
    "        return count_binning_N(df, column_name, bins, method, target)[0]\n",
    "    elif column_name in binning_choice_S1:\n",
    "        return 'Do nothing'\n",
    "    else:\n",
    "        return count_binning_other(df, column_name, bins, method, target)[0]\n",
    "    \n",
    "def distribution_plotting(df, column_name, bins = None, method = None, target = None): #Do not use for column in binning_choice_S1\n",
    "    list_of_bin = binning_list_final(df, column_name, bins, method, target)\n",
    "    distribution = count_binning_final(df, column_name, bins, method, target)\n",
    "    fig = go.Figure()\n",
    "    for k in range(len(list_of_bin)):\n",
    "        fig.add_trace(\n",
    "                    go.Bar(\n",
    "                            x = list_of_month,\n",
    "                            y = distribution[k],\n",
    "                            name = list_of_bin[k]\n",
    "                            )\n",
    "                    )\n",
    "\n",
    "        fig.update_layout(\n",
    "        title=\"Distribution of {}\".format(column_name),\n",
    "        xaxis_title=\"Month\",\n",
    "        yaxis_title=\"Percent\",\n",
    "    \n",
    "        font=dict(family=\"Calibri\", size=12, color=\"Black\")\n",
    "                )\n",
    "\n",
    "        fig.update_layout(barmode='stack', xaxis={'categoryorder':'category ascending'})\n",
    "\n",
    "        fig.update_layout(dragmode='pan', hovermode='closest', hoverdistance=10)\n",
    "        fig.update_layout(legend=dict(x=1, y=1, bgcolor='rgba(0,0,0,0)'))\n",
    "        fig.update_layout(margin=dict(b=20, t=25, l=0, r=0))\n",
    "        fig.update_xaxes(showspikes=True, spikemode='across',\n",
    "                              spikesnap='cursor', spikedash='dot')\n",
    "        fig.update_yaxes(showspikes=True, spikemode='across',\n",
    "                              spikesnap='cursor', spikedash='dot')\n",
    "        fig.update_xaxes(showgrid=True)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e57b8fa",
   "metadata": {},
   "source": [
    "# Del_Mob Line Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "072ea68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_mob_N(df, column_name, bins = None, method = None, target = None): #Only for type N\n",
    "    ## Get table\n",
    "    list_of_bin = binning_list_plus(df, column_name, bins, method, target)\n",
    "    df2 = df[['app_id', 'applied_month_xsell', column_name]]\n",
    "    df2.loc[:,'target'] = df[target].replace('unknown', 0) #It is not necessary\n",
    "    \n",
    "    list_of_total_by_month = df2.groupby('applied_month_xsell', dropna = False)\\\n",
    "                                .aggregate({'target': 'count'})\\\n",
    "                                .reset_index()\\\n",
    "                                ['target']\\\n",
    "                                .cumsum().tolist()\n",
    "    df3 = df2[['app_id','applied_month_xsell', column_name, 'target']]\\\n",
    "                            .groupby(['applied_month_xsell', column_name], dropna = False)\\\n",
    "                            .aggregate({'target': 'sum'})\\\n",
    "                            .reset_index()\n",
    "    df4 = df3.pivot(index = 'applied_month_xsell', columns = column_name, values = 'target')\\\n",
    "                    .fillna(value = 0)\n",
    "            \n",
    "    ## Get percent\n",
    "    line_percent = []\n",
    "    for binning in list_of_bin:\n",
    "        a = (np.array(df4[binning].cumsum().tolist())/(np.array(list_of_total_by_month)))\n",
    "        df4.loc[:, binning] = a\n",
    "        line_percent_bin = df4[binning].tolist()\n",
    "        line_percent.append(line_percent_bin)\n",
    "    if 'unknown' in df4.columns.to_list():\n",
    "        line_percent.append(\n",
    "                            (\n",
    "                                np.array(df4['unknown'].cumsum().tolist())/(np.array(list_of_total_by_month))\n",
    "                            ).tolist()\n",
    "                           )\n",
    "        df4.loc[:, 'unknown'] = line_percent[-1]\n",
    "    else:\n",
    "        df4.loc[:,'unknown'] = [0]*len(df4)\n",
    "        line_percent.append([0]*len(df4))    \n",
    "    return [df4, line_percent]\n",
    "\n",
    "def del_mob_other(df, column_name, bins = None, method = None, target = None): #For type S_2, Y_int, Y_float\n",
    "    ## Get table\n",
    "    list_of_bin = binning_list_plus(df, column_name, bins, method, target)\n",
    "    df2 = df[['app_id', 'applied_month_xsell', column_name]]\n",
    "    df2.loc[:, 'ranking'] = ranking_list(df, column_name, bins, method, target)\n",
    "    df2.loc[:, 'target'] = df[target].replace('unknown', 0) #It is not necessary\n",
    "    \n",
    "    list_of_total_by_month = df2.groupby('applied_month_xsell', dropna = False)\\\n",
    "                                .aggregate({'target': 'count'})\\\n",
    "                                .reset_index()\\\n",
    "                                ['target']\\\n",
    "                                .cumsum().tolist()\n",
    "    df3 = df2.groupby(['applied_month_xsell', 'ranking'], dropna = False)\\\n",
    "             .aggregate({'target': 'sum'})\\\n",
    "             .reset_index()\n",
    "\n",
    "    df4 = df3.pivot(index = 'applied_month_xsell', columns = 'ranking', values = 'target')\\\n",
    "                    .fillna(value = 0)\n",
    "               \n",
    "    ## Get percent\n",
    "    line_percent = []\n",
    "    for binning in list_of_bin:\n",
    "        a = (np.array(df4[binning].cumsum().tolist())/(np.array(list_of_total_by_month)))\n",
    "        df4.loc[:, binning] = a\n",
    "        line_percent_bin = df4[binning].tolist()\n",
    "        line_percent.append(line_percent_bin)\n",
    "    if 'unknown' in df4.columns.to_list():\n",
    "        line_percent.append(\n",
    "                            (\n",
    "                                np.array(df4['unknown'].cumsum().tolist())/(np.array(list_of_total_by_month))\n",
    "                            ).tolist()\n",
    "                           )\n",
    "        df4.loc[:, 'unknown'] = line_percent[-1]\n",
    "    else:\n",
    "        df4.loc[:, 'unknown'] = [0]*len(df4)\n",
    "        line_percent.append([0]*len(df4))    \n",
    "    return [df4, line_percent]\n",
    "\n",
    "def del_mob_percent_final(df, column_name, bins = None, method = None, target = None):\n",
    "    if (column_name in binning_choice_N) or (column_name in optimal_binning_N):\n",
    "        return del_mob_N(df, column_name, bins, method, target)[1]\n",
    "    elif column_name in binning_choice_S1:\n",
    "        return 'Do nothing'\n",
    "    else:\n",
    "        return del_mob_other(df, column_name, bins, method, target)[1]\n",
    "\n",
    "def del_mob_table_final(df, column_name, bins = None, method = None, target = None):\n",
    "    if (column_name in binning_choice_N) or (column_name in optimal_binning_N):\n",
    "        return del_mob_N(df, column_name, bins, method, target)[0]\n",
    "    elif column_name in binning_choice_S1:\n",
    "        return 'Do nothing'\n",
    "    else:\n",
    "        return del_mob_other(df, column_name, bins, method, target)[0]\n",
    "    \n",
    "def del_mob_plotting(df, column_name, bins = None, method = None, target = None): #Do not use for column in binning_choice_S1\n",
    "    list_of_bin = binning_list_final(df, column_name, bins, method, target)\n",
    "    del_mob = del_mob_percent_final(df, column_name, bins, method, target)\n",
    "    fig = go.Figure()\n",
    "    for k in range(len(list_of_bin)):\n",
    "        fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                            x = list_of_month,\n",
    "                            y = del_mob[k],\n",
    "                            name = list_of_bin[k]\n",
    "                            )\n",
    "                    )\n",
    "\n",
    "        fig.update_layout(\n",
    "        title=\"{} % by {}\".format(target, column_name),\n",
    "        xaxis_title=\"Month\",\n",
    "        yaxis_title=\"Percent\",\n",
    "    \n",
    "        font=dict(family=\"Calibri\", size=12, color=\"Black\")\n",
    "                )\n",
    "\n",
    "        fig.update_layout(barmode='stack', xaxis={'categoryorder':'category ascending'})\n",
    "\n",
    "        fig.update_layout(dragmode='pan', hovermode='closest', hoverdistance=10)\n",
    "        fig.update_layout(legend=dict(x=1, y=1, bgcolor='rgba(0,0,0,0)'))\n",
    "        fig.update_layout(margin=dict(b=20, t=25, l=0, r=0))\n",
    "        fig.update_xaxes(showspikes=True, spikemode='across',\n",
    "                              spikesnap='cursor', spikedash='dot')\n",
    "        fig.update_yaxes(showspikes=True, spikemode='across',\n",
    "                              spikesnap='cursor', spikedash='dot')\n",
    "        fig.update_xaxes(showgrid=True)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6434aed0",
   "metadata": {},
   "source": [
    "# Binning Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b445693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_two_lines_cut(list1, list2):\n",
    "    diff = np.array(list1) - np.array(list2)\n",
    "    diff = diff.tolist()\n",
    "    sign = []\n",
    "    for i in range(len(diff)-1):\n",
    "        if (diff[i]*diff[i+1] <= 0):\n",
    "            sign.append(1)\n",
    "        else:\n",
    "            sign.append(0)\n",
    "    if 1 in sign:\n",
    "        return 'Need to be re-binned (cut together)'\n",
    "    else:\n",
    "        return 'To be considered in the next step'\n",
    "    \n",
    "def do_two_lines_close(list1, list2):\n",
    "    close1 = 0.005\n",
    "    close2 = 0.003\n",
    "    diff = np.absolute(np.array(list1) - np.array(list2))\n",
    "    diff = diff.tolist()\n",
    "    if max(diff) <= close1:\n",
    "        if min(diff) >= close2:\n",
    "            return 'No need'\n",
    "        else:\n",
    "            return 'Need to be re-binned (close to each other)'\n",
    "    else:\n",
    "        if min(diff) >= close1:\n",
    "            return 'No need'\n",
    "        else:\n",
    "            return 'To be considered in the next step'\n",
    "    \n",
    "def do_two_lines_parallel(list1, list2):\n",
    "    unit = 0.01\n",
    "    angle_cut_off = m.pi/12\n",
    "    diff = np.absolute(np.array(list1) - np.array(list2))\n",
    "    diff = diff.tolist()\n",
    "    angle = []\n",
    "    for i in range(len(diff)-1):\n",
    "        tangent_of_angle = abs(diff[i+1]-diff[i])/unit\n",
    "        angle.append(m.atan(tangent_of_angle))\n",
    "    mean = sum(angle)/len(angle)\n",
    "    if mean <= angle_cut_off:\n",
    "        return 'No need'\n",
    "    else:\n",
    "        return 'To be considered in the next step'\n",
    "    \n",
    "def do_two_lines_asymptote(list1, list2):\n",
    "    unit = 0.01\n",
    "    shift_forward = 6\n",
    "    a = len(list1)\n",
    "    b = len(list2)\n",
    "    list3 = [] + list1 # Set list3 = list1 will cause error\n",
    "    list4 = [] + list2\n",
    "    for i in range(shift_forward):\n",
    "        list3.append((list3[a-1+i] - list3[a-2+i] + list3[a-2+i] - list3[a-3+i])/2 * unit + list3[a-1+i])\n",
    "        list4.append((list4[b-1+i] - list4[b-2+i] + list4[b-2+i] - list4[b-3+i])/2 * unit + list4[b-1+i])\n",
    "    if do_two_lines_cut(list3, list4) == 'Need to be re-binned':\n",
    "        return 'Need to be re-binned (asymptote)'\n",
    "    else:\n",
    "        return 'To be considered manually'\n",
    "        \n",
    "def do_two_lines_re_bin(list1, list2):\n",
    "    if do_two_lines_cut(list1, list2) == 'Need to be re-binned (cut together)':\n",
    "        return 'Y1'\n",
    "    else:\n",
    "        if do_two_lines_close(list1, list2) == 'Need to be re-binned (close to each other)':\n",
    "            return 'Y2'\n",
    "        elif do_two_lines_close(list1, list2) == 'No need':\n",
    "            return 'N'\n",
    "        else:\n",
    "            if do_two_lines_parallel(list1, list2) == 'No need':\n",
    "                return 'N'\n",
    "            else:\n",
    "                if do_two_lines_asymptote(list1, list2) == 'Need to be re-binned (asymptote)':\n",
    "                    return 'Y3'\n",
    "                else:\n",
    "                    return 'Considered'\n",
    "\n",
    "def is_feature_need_to_be_rebinned(df, column_name, bins = None, method = None, target = None):\n",
    "    list_of_bin = binning_list_final(df, column_name, bins, method, target)\n",
    "    del_mob = del_mob_percent_final(df, column_name, bins, method, target)\n",
    "    re_bin = []\n",
    "    for k in range(len(list_of_bin)-2):\n",
    "        l = 1\n",
    "        while (k+l) < len(list_of_bin) -1:\n",
    "            re_bin.append(do_two_lines_re_bin(del_mob[k], del_mob[k+l]))\n",
    "            l = l+1\n",
    "    if ('Y1' in re_bin) or ('Y2' in re_bin) or ('Y3' in re_bin):\n",
    "        return 'Y'\n",
    "    else:\n",
    "        if ('Considered' in re_bin):\n",
    "            return 'Considered'\n",
    "        else:\n",
    "            return 'N'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389ccb44",
   "metadata": {},
   "source": [
    "# Group categorical bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc80bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_categorical_values(df, column_name, bins = None, method = None, target = None):\n",
    "    distribution = distribution_table_final(df, column_name, bins, method, target)\n",
    "    del_mob = del_mob_table_final(df, column_name, bins, method, target)\n",
    "    df_temp1 = pd.DataFrame()\n",
    "    df_temp1['applied_month_xsell'] = list_of_month\n",
    "    df_temp2 = pd.DataFrame()\n",
    "    df_temp2['applied_month_xsell'] = list_of_month\n",
    "    for k in range(len(bins)):\n",
    "        new_bin1 = np.array([0]*len(df_temp1))\n",
    "        new_bin2 = np.array([0]*len(df_temp2))\n",
    "        for l in bins[k]:\n",
    "            new_bin1 = new_bin1 + np.array(distribution[l])\n",
    "            new_bin2 = new_bin2 + np.array(del_mob[l])\n",
    "        df_temp1[str(bins[k])] = new_bin1\n",
    "        df_temp2[str(bins[k])] = new_bin2\n",
    "    return [df_temp1, df_temp2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3631a50",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'del90_mob12_app'\n",
    "bin_list = [\n",
    "            0\n",
    "            ]\n",
    "i = 5\n",
    "print(is_feature_need_to_be_rebinned(df, list_column[i], bins = bin_list, target = 'del90_mob12_app'))\n",
    "print(is_feature_need_to_be_rebinned(df, list_column[i], method = 'optimal_binning', target = 'del90_mob12_app'))\n",
    "print(is_feature_need_to_be_rebinned(df, list_column[i], method = 'statistics', target = 'del90_mob12_app'))\n",
    "\n",
    "distribution_plotting(df, list_column[i], bins = bin_list , method = None).show()\n",
    "del_mob_plotting(df, list_column[i], bins = bin_list , method = None, target = 'del90_mob12_app').show()\n",
    "\n",
    "distribution_plotting(df, list_column[i], method = 'optimal_binning', target = 'del90_mob12_app').show()\n",
    "del_mob_plotting(df, list_column[i], method = 'optimal_binning', target = 'del90_mob12_app').show()\n",
    "\n",
    "distribution_plotting(df, list_column[i], method = 'statistics').show()\n",
    "del_mob_plotting(df, list_column[i], method = 'statistics', target = 'del90_mob12_app').show()\n",
    "\n",
    "bin2 = [['<= 0'], ['<= 1', '> 1'], ['unknown']]\n",
    "group_categorical_values(df, list_column[4], bins = bin2, method = 'optimal_binning', target = 'del90_mob12_app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1540493b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 113743 entries, 0 to 113742\n",
      "Data columns (total 57 columns):\n",
      " #   Column                      Non-Null Count   Dtype         \n",
      "---  ------                      --------------   -----         \n",
      " 0   cif_nb                      113743 non-null  int64         \n",
      " 1   app_id                      113743 non-null  object        \n",
      " 2   applied_date                113743 non-null  datetime64[ns]\n",
      " 3   product_group_dt2           113743 non-null  object        \n",
      " 4   max_dpdall_ref              104599 non-null  float64       \n",
      " 5   max_dpd_ref                 104599 non-null  float64       \n",
      " 6   prev_approved               113743 non-null  int64         \n",
      " 7   prev_rejected               113743 non-null  int64         \n",
      " 8   good_response_ratio         113743 non-null  float64       \n",
      " 9   bad_response_ratio          113743 non-null  float64       \n",
      " 10  max_nationalid_date         113743 non-null  float64       \n",
      " 11  min_nationalid_date         113743 non-null  float64       \n",
      " 12  mode_appeducation           113743 non-null  object        \n",
      " 13  mode_appfamilystatus        113743 non-null  object        \n",
      " 14  mode_dop_maingoodscategory  113743 non-null  object        \n",
      " 15  mode_appdisbchan            113743 non-null  object        \n",
      " 16  avg_appinterest             113743 non-null  float64       \n",
      " 17  max_appwperiod              113743 non-null  int64         \n",
      " 18  attempt_3m_cnt              113743 non-null  int64         \n",
      " 19  attempt_6m_cnt              113743 non-null  int64         \n",
      " 20  attempt_9m_cnt              113743 non-null  int64         \n",
      " 21  attempt_12m_cnt             113743 non-null  int64         \n",
      " 22  attempt_24m_cnt             113743 non-null  int64         \n",
      " 23  app_id_xsell                113743 non-null  object        \n",
      " 24  applied_month               113743 non-null  object        \n",
      " 25  applied_date_xsell          113743 non-null  datetime64[ns]\n",
      " 26  applied_month_xsell         113743 non-null  object        \n",
      " 27  del30_mob12_app             113743 non-null  int64         \n",
      " 28  del90_mob12_app             113743 non-null  int64         \n",
      " 29  max_appwperiodg             113743 non-null  int64         \n",
      " 30  max_appfmqnty               74246 non-null   float64       \n",
      " 31  max_appchildqnty            74232 non-null   float64       \n",
      " 32  max_life_time               113743 non-null  int64         \n",
      " 33  months_dpd_0_cus_journey    113488 non-null  float64       \n",
      " 34  months_dpd_10_cus_journey   113488 non-null  float64       \n",
      " 35  months_dpd_30_cus_journey   113488 non-null  float64       \n",
      " 36  dpdall_ever                 113743 non-null  int64         \n",
      " 37  dpdall_3mob                 113743 non-null  int64         \n",
      " 38  dpdall_4_6mob               113743 non-null  int64         \n",
      " 39  dpdall_7_9mob               113743 non-null  int64         \n",
      " 40  dpdall_10_12mob             113743 non-null  int64         \n",
      " 41  avg_enr_ratio_ever          113488 non-null  float64       \n",
      " 42  avg_enr_ratio_last_3m       113488 non-null  float64       \n",
      " 43  avg_enr_ratio_last_3_6m     112354 non-null  float64       \n",
      " 44  avg_enr_ratio_last_6_12m    110735 non-null  float64       \n",
      " 45  avg_enr_ratio_last_12_24m   104298 non-null  float64       \n",
      " 46  contact_client_rate         113743 non-null  float64       \n",
      " 47  contact_client_rate_3m      113743 non-null  float64       \n",
      " 48  contact_client_rate_6m      113743 non-null  float64       \n",
      " 49  contact_client_rate_9m      113743 non-null  float64       \n",
      " 50  contact_client_rate_12m     113743 non-null  float64       \n",
      " 51  avg_fmincome                19043 non-null   float64       \n",
      " 52  inctoexp                    110088 non-null  float64       \n",
      " 53  holiday_flag                113743 non-null  int64         \n",
      " 54  double_day_flag             113743 non-null  int64         \n",
      " 55  day_of_week                 113743 non-null  object        \n",
      " 56  weekend_flag                113743 non-null  int64         \n",
      "dtypes: datetime64[ns](2), float64(24), int64(21), object(10)\n",
      "memory usage: 49.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f89599f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
